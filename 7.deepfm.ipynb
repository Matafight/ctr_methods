{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self,features_size,embedding_size):\n",
    "        super(DeepFM,self).__init__()\n",
    "        \n",
    "        self.features_size = features_size\n",
    "        self.field_size = len(features_size)\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        ## FM first order embedding\n",
    "        self.first_order_embeddings = nn.ModuleList([nn.Embedding(feature_size,1) for feature_size in features_size])\n",
    "        ## FM second order embedding\n",
    "        self.second_order_embeddings = nn.ModuleList([nn.Embedding(feature_size,self.embedding_size) for feature_size in features_size])\n",
    "        ## Deep part\n",
    "        self.dense_dim = self.field_size * self.embedding_size\n",
    "        self.fc_dim = 500\n",
    "        \n",
    "        \n",
    "        self.fc_layer1 = nn.Sequential(\n",
    "                nn.Linear(self.dense_dim,self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.LeakyReLU(0.2, inplace=True)     \n",
    "                )\n",
    " \n",
    "        self.fc_layer2 = nn.Sequential(\n",
    "                nn.Linear(self.fc_dim,self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim), \n",
    "                nn.LeakyReLU(0.2, inplace=True)     \n",
    "                )\n",
    "        \n",
    "    \n",
    "    def forward(self,xi,vi):\n",
    "        ## input shape: bacth_size * field_size\n",
    "        \n",
    "        first_order_embeddings = self.first_order_embeddings\n",
    "        second_order_embeddings = self.second_order_embeddings\n",
    "        field_size = self.field_size\n",
    "        ## FM first order part\n",
    "#         pdb.set_trace()\n",
    "            \n",
    "        first_order_output_tmp1 = [ (first_order_embeddings[i](xi[:,i]).t()*vi[:,i]).t() for i in range(field_size)]\n",
    "        first_order_output_tmp2 = torch.cat(first_order_output_tmp1,axis=1)\n",
    "        first_order_output = torch.sum(first_order_output_tmp2,axis=1)\n",
    "        \n",
    "        ## FM second order part\n",
    "        second_order_output_tmp1 = [(second_order_embeddings[i](xi[:,i]).t()*vi[:,i]).t() for i in range(field_size) ]\n",
    "        ## 计算第一个二阶项, 固定 field不变\n",
    "        sum_sec_order_output  = sum(second_order_output_tmp1)\n",
    "        squared_sec_order_output = sum_sec_order_output*sum_sec_order_output\n",
    "        sum_squared_sec_order_output = torch.sum(squared_sec_order_output,axis=1)/2\n",
    "        ## 计算第二个二阶项\n",
    "        squared_sec_list = [item*item for item in second_order_output_tmp1]\n",
    "        sum_squared_sec = sum(squared_sec_list)\n",
    "        sum_sum_squared_sec = torch.sum(sum_squared_sec,axis=1)/2\n",
    "        ## finally\n",
    "        second_order_output = sum_squared_sec_order_output-sum_sum_squared_sec\n",
    "        \n",
    "        ## Deep part\n",
    "        dense_input = torch.cat(second_order_output_tmp1,axis=1)\n",
    "        fc1_output = self.fc_layer1(dense_input)\n",
    "        fc2_output = self.fc_layer2(fc1_output)\n",
    "        \n",
    "        ## sum all together\n",
    "        sum_all = first_order_output+second_order_output+ torch.sum(fc2_output,axis=1)\n",
    "        return sum_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## dataloader\n",
    "import pickle\n",
    "with open('./processed/deepfm_training.pickle','rb') as fh: \n",
    "    total_data = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xi =np.array(total_data['xi'],dtype=int)\n",
    "vi = np.array(total_data['vi'],dtype=int)\n",
    "label = np.array(total_data['label'],dtype=int)\n",
    "## convert numpy.ndarray to torch tensor object\n",
    "xi = torch.tensor(xi,dtype=torch.int)\n",
    "vi = torch.tensor(vi,dtype=torch.int)\n",
    "label=torch.tensor(label,dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FM_Dataset(TensorDataset):\n",
    "    def __init__(self,xi,vi,y=None):\n",
    "        super(FM_Dataset,self).__init__()\n",
    "        self.xi=xi\n",
    "        self.vi=vi\n",
    "        self.y=y\n",
    "    def __len__(self):\n",
    "        return self.xi.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        if self.y is not None: \n",
    "            sample = {'xi':self.xi[index],'vi':self.vi[index],'y':self.y[index]}\n",
    "        else:\n",
    "            sample = {'xi':self.xi[index],'vi':self.vi[index]}\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = FM_Dataset(xi,vi,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "dataloader = DataLoader(dataset,shuffle=True,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##先看数据的feature_size 是多少？\n",
    "## 这里只是针对这小部分数据取的feature_size\n",
    "features_size=[]\n",
    "for col in range(7):\n",
    "    features_size.append(int(max(xi[:,col]))+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048407"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(features_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch2\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 7])\n"
     ]
    }
   ],
   "source": [
    "# ## 定义损失函数\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "\n",
    "features_size=features_size\n",
    "embedding_size=100\n",
    "# ## 定义模型\n",
    "fm_model=DeepFM(features_size,embedding_size)\n",
    "# 试一下forward\n",
    "for data in dataloader:\n",
    "    xi = torch.tensor(data['xi'],dtype=torch.long)\n",
    "    vi = torch.tensor(data['vi'],dtype=torch.float32)\n",
    "    print(xi.shape)\n",
    "    label = torch.tensor(data['y'],dtype=torch.long)\n",
    "    print(vi.shape)\n",
    "    output=fm_model(xi,vi)\n",
    "    \n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (pytorch2)",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
